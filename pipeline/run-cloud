#!/bin/bash

set -e

# Parse command line arguments
WORK_DIR=gs://badgewell-reco
PROJECT=careerograph-e9963
REGION=us-central1
while [[ $# -gt 0 ]]; do
  case $1 in
    --work-dir)
      WORK_DIR=$2
      shift
      ;;
    --project)
      PROJECT=$2
      shift
      ;;
    --region)
      REGION=$2
      shift
      ;;
    *)
      echo "error: unrecognized argument $1"
      exit 1
      ;;
  esac
  shift
done

if [[ -z $WORK_DIR ]]; then
  echo "error: argument --work-dir is required"
  exit 1
fi

if [[ $WORK_DIR != gs://* ]]; then
  echo "error: --work-dir must be a Google Cloud Storage path"
  echo "       example: gs://your-bucket/cloudml-samples/molecules"
  exit 1
fi

if [[ -z $PROJECT ]]; then
  echo 'error: --project is required to run in Google Cloud Platform.'
  exit 1
fi

# Wrapper function to print the command being run
function run {
  echo "$ $@"
  "$@"
}

# Preprocess the datasets using Apache Beam's DataflowRunner
echo '>> Preprocessing'
run python pipeline.py \
  --project $PROJECT \
  --runner DataflowRunner \
  --staging_location $WORK_DIR/staging \
  --temp_location $WORK_DIR/temp \
  --template_location $WORK_DIR/templates/pipeline_template \
  --setup_file ./setup.py \
  --input $WORK_DIR/data \
  --output $WORK_DIR/output \
  --environment cloud
echo ''


